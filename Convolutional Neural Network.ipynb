{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatx=float32 python cifar10_cnn.py\n",
    "\n",
    "It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n",
    "(it's still underfitting at that point, though)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimensions:(32, 32, 3)\n",
      "Target Dimensions: (10,)\n",
      "Target: [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at our data\n",
    "print(\"Input Dimensions:\" + str(x_train[0].shape))\n",
    "print(\"Target Dimensions: \" + str(y_train[0].shape))\n",
    "print(\"Target: \" + str(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Data Augmentation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Train the model.\n",
      "Epoch 1/200\n",
      "1562/1562 [==============================] - 19s - loss: 1.8453 - acc: 0.3222 - val_loss: 1.5459 - val_acc: 0.4392\n",
      "Epoch 2/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.5666 - acc: 0.4280 - val_loss: 1.3721 - val_acc: 0.5113\n",
      "Epoch 3/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.4349 - acc: 0.4825 - val_loss: 1.2648 - val_acc: 0.5531\n",
      "Epoch 4/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.3453 - acc: 0.5206 - val_loss: 1.1819 - val_acc: 0.5771\n",
      "Epoch 5/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.2695 - acc: 0.5507 - val_loss: 1.0975 - val_acc: 0.6143\n",
      "Epoch 6/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.2076 - acc: 0.5722 - val_loss: 1.0462 - val_acc: 0.6346\n",
      "Epoch 7/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1564 - acc: 0.5914 - val_loss: 1.0292 - val_acc: 0.6374\n",
      "Epoch 8/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1199 - acc: 0.6057 - val_loss: 0.9657 - val_acc: 0.6555\n",
      "Epoch 9/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0817 - acc: 0.6174 - val_loss: 0.9573 - val_acc: 0.6600\n",
      "Epoch 10/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0465 - acc: 0.6318 - val_loss: 0.8938 - val_acc: 0.6834\n",
      "Epoch 11/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0189 - acc: 0.6412 - val_loss: 0.8688 - val_acc: 0.6960\n",
      "Epoch 12/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9934 - acc: 0.6501 - val_loss: 0.8645 - val_acc: 0.6989\n",
      "Epoch 13/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9758 - acc: 0.6581 - val_loss: 0.8355 - val_acc: 0.7096\n",
      "Epoch 14/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9506 - acc: 0.6660 - val_loss: 0.8132 - val_acc: 0.7161\n",
      "Epoch 15/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9380 - acc: 0.6718 - val_loss: 0.8323 - val_acc: 0.7107\n",
      "Epoch 16/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9174 - acc: 0.6773 - val_loss: 0.8080 - val_acc: 0.7179\n",
      "Epoch 17/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9045 - acc: 0.6841 - val_loss: 0.8038 - val_acc: 0.7222\n",
      "Epoch 18/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8950 - acc: 0.6855 - val_loss: 0.7916 - val_acc: 0.7293\n",
      "Epoch 19/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8829 - acc: 0.6935 - val_loss: 0.7401 - val_acc: 0.7457\n",
      "Epoch 20/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8707 - acc: 0.6967 - val_loss: 0.7572 - val_acc: 0.7371\n",
      "Epoch 21/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8531 - acc: 0.7034 - val_loss: 0.7381 - val_acc: 0.7452\n",
      "Epoch 22/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8497 - acc: 0.7035 - val_loss: 0.7452 - val_acc: 0.7451\n",
      "Epoch 23/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8449 - acc: 0.7064 - val_loss: 0.7097 - val_acc: 0.7559\n",
      "Epoch 24/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8414 - acc: 0.7108 - val_loss: 0.7189 - val_acc: 0.7550\n",
      "Epoch 25/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8336 - acc: 0.7114 - val_loss: 0.7194 - val_acc: 0.7604\n",
      "Epoch 26/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8268 - acc: 0.7150 - val_loss: 0.7157 - val_acc: 0.7570\n",
      "Epoch 27/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8219 - acc: 0.7187 - val_loss: 0.6937 - val_acc: 0.7605\n",
      "Epoch 28/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8139 - acc: 0.7201 - val_loss: 0.7089 - val_acc: 0.7626\n",
      "Epoch 29/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8103 - acc: 0.7211 - val_loss: 0.6699 - val_acc: 0.7707\n",
      "Epoch 30/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8088 - acc: 0.7216 - val_loss: 0.6734 - val_acc: 0.7704\n",
      "Epoch 31/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7969 - acc: 0.7264 - val_loss: 0.6788 - val_acc: 0.7676\n",
      "Epoch 32/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7971 - acc: 0.7290 - val_loss: 0.6760 - val_acc: 0.7738\n",
      "Epoch 33/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7921 - acc: 0.7284 - val_loss: 0.6723 - val_acc: 0.7763\n",
      "Epoch 34/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7871 - acc: 0.7301 - val_loss: 0.6712 - val_acc: 0.7691\n",
      "Epoch 35/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7914 - acc: 0.7295 - val_loss: 0.6764 - val_acc: 0.7732\n",
      "Epoch 36/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7784 - acc: 0.7319 - val_loss: 0.6762 - val_acc: 0.7732\n",
      "Epoch 37/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7786 - acc: 0.7352 - val_loss: 0.6529 - val_acc: 0.7818\n",
      "Epoch 38/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7707 - acc: 0.7383 - val_loss: 0.6790 - val_acc: 0.7690\n",
      "Epoch 39/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7706 - acc: 0.7364 - val_loss: 0.6592 - val_acc: 0.7808\n",
      "Epoch 40/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7728 - acc: 0.7363 - val_loss: 0.6473 - val_acc: 0.7845\n",
      "Epoch 41/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7642 - acc: 0.7412 - val_loss: 0.6875 - val_acc: 0.7719\n",
      "Epoch 42/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7676 - acc: 0.7375 - val_loss: 0.6447 - val_acc: 0.7858\n",
      "Epoch 43/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7606 - acc: 0.7424 - val_loss: 0.6586 - val_acc: 0.7798\n",
      "Epoch 44/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7639 - acc: 0.7394 - val_loss: 0.6654 - val_acc: 0.7728\n",
      "Epoch 45/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7583 - acc: 0.7406 - val_loss: 0.6552 - val_acc: 0.7823\n",
      "Epoch 46/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7591 - acc: 0.7425 - val_loss: 0.6294 - val_acc: 0.7893\n",
      "Epoch 47/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7588 - acc: 0.7439 - val_loss: 0.6490 - val_acc: 0.7770\n",
      "Epoch 48/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7545 - acc: 0.7455 - val_loss: 0.6403 - val_acc: 0.7834\n",
      "Epoch 49/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7503 - acc: 0.7442 - val_loss: 0.6551 - val_acc: 0.7800\n",
      "Epoch 50/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7531 - acc: 0.7466 - val_loss: 0.6352 - val_acc: 0.7854\n",
      "Epoch 51/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7514 - acc: 0.7450 - val_loss: 0.6484 - val_acc: 0.7843\n",
      "Epoch 52/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7535 - acc: 0.7431 - val_loss: 0.6130 - val_acc: 0.7948\n",
      "Epoch 53/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7444 - acc: 0.7483 - val_loss: 0.6500 - val_acc: 0.7818\n",
      "Epoch 54/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7432 - acc: 0.7496 - val_loss: 0.6320 - val_acc: 0.7891\n",
      "Epoch 55/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7478 - acc: 0.7472 - val_loss: 0.6685 - val_acc: 0.7907\n",
      "Epoch 56/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7392 - acc: 0.7505 - val_loss: 0.6582 - val_acc: 0.7790\n",
      "Epoch 57/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7472 - acc: 0.7476 - val_loss: 0.6295 - val_acc: 0.7895\n",
      "Epoch 58/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7390 - acc: 0.7503 - val_loss: 0.6766 - val_acc: 0.7880\n",
      "Epoch 59/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7401 - acc: 0.7528 - val_loss: 0.6096 - val_acc: 0.7946\n",
      "Epoch 60/200\n",
      "1562/1562 [==============================] - 18s - loss: 0.7394 - acc: 0.7526 - val_loss: 0.6119 - val_acc: 0.7958\n",
      "Epoch 61/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7442 - acc: 0.7517 - val_loss: 0.6200 - val_acc: 0.7970\n",
      "Epoch 62/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7412 - acc: 0.7510 - val_loss: 0.6393 - val_acc: 0.7879\n",
      "Epoch 63/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7403 - acc: 0.7513 - val_loss: 0.6211 - val_acc: 0.7969\n",
      "Epoch 64/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7336 - acc: 0.7530 - val_loss: 0.6269 - val_acc: 0.8015\n",
      "Epoch 65/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7399 - acc: 0.7510 - val_loss: 0.6410 - val_acc: 0.7879\n",
      "Epoch 66/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7398 - acc: 0.7529 - val_loss: 0.6364 - val_acc: 0.7912\n",
      "Epoch 67/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7410 - acc: 0.7515 - val_loss: 0.6797 - val_acc: 0.7924\n",
      "Epoch 68/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7413 - acc: 0.7517 - val_loss: 0.6512 - val_acc: 0.7941\n",
      "Epoch 69/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7369 - acc: 0.7549 - val_loss: 0.6348 - val_acc: 0.7899\n",
      "Epoch 70/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7417 - acc: 0.7527 - val_loss: 0.6144 - val_acc: 0.7943\n",
      "Epoch 71/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7378 - acc: 0.7542 - val_loss: 0.7468 - val_acc: 0.7730\n",
      "Epoch 72/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7427 - acc: 0.7519 - val_loss: 0.6155 - val_acc: 0.8016\n",
      "Epoch 73/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7398 - acc: 0.7541 - val_loss: 0.6427 - val_acc: 0.7872\n",
      "Epoch 74/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7322 - acc: 0.7569 - val_loss: 0.6772 - val_acc: 0.7821\n",
      "Epoch 75/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7366 - acc: 0.7544 - val_loss: 0.7097 - val_acc: 0.7805\n",
      "Epoch 76/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7443 - acc: 0.7525 - val_loss: 0.6095 - val_acc: 0.7980\n",
      "Epoch 77/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7367 - acc: 0.7550 - val_loss: 0.6402 - val_acc: 0.7851\n",
      "Epoch 78/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7410 - acc: 0.7537 - val_loss: 0.6824 - val_acc: 0.7874\n",
      "Epoch 79/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7413 - acc: 0.7541 - val_loss: 0.6190 - val_acc: 0.7965\n",
      "Epoch 80/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7445 - acc: 0.7529 - val_loss: 0.6775 - val_acc: 0.7836\n",
      "Epoch 81/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7390 - acc: 0.7546 - val_loss: 0.7058 - val_acc: 0.7730\n",
      "Epoch 82/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7428 - acc: 0.7526 - val_loss: 0.6796 - val_acc: 0.7730\n",
      "Epoch 83/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7439 - acc: 0.7517 - val_loss: 0.6360 - val_acc: 0.7907\n",
      "Epoch 84/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7479 - acc: 0.7528 - val_loss: 0.6298 - val_acc: 0.7983\n",
      "Epoch 85/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7477 - acc: 0.7513 - val_loss: 0.6685 - val_acc: 0.7841\n",
      "Epoch 86/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7503 - acc: 0.7498 - val_loss: 0.6033 - val_acc: 0.7957\n",
      "Epoch 87/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7441 - acc: 0.7521 - val_loss: 0.7822 - val_acc: 0.7628\n",
      "Epoch 88/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7497 - acc: 0.7509 - val_loss: 0.6330 - val_acc: 0.7934\n",
      "Epoch 89/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7509 - acc: 0.7513 - val_loss: 0.6881 - val_acc: 0.7743\n",
      "Epoch 90/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7545 - acc: 0.7518 - val_loss: 0.6485 - val_acc: 0.7940\n",
      "Epoch 91/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7517 - acc: 0.7511 - val_loss: 0.6529 - val_acc: 0.7926\n",
      "Epoch 92/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7624 - acc: 0.7465 - val_loss: 0.6548 - val_acc: 0.7965\n",
      "Epoch 93/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7560 - acc: 0.7506 - val_loss: 0.6883 - val_acc: 0.7922\n",
      "Epoch 94/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7574 - acc: 0.7494 - val_loss: 0.6942 - val_acc: 0.7873\n",
      "Epoch 95/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7616 - acc: 0.7483 - val_loss: 0.7418 - val_acc: 0.7708\n",
      "Epoch 96/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7596 - acc: 0.7489 - val_loss: 0.6511 - val_acc: 0.7856\n",
      "Epoch 97/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7643 - acc: 0.7483 - val_loss: 0.6132 - val_acc: 0.7941\n",
      "Epoch 98/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7578 - acc: 0.7521 - val_loss: 0.7040 - val_acc: 0.7758\n",
      "Epoch 99/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7692 - acc: 0.7457 - val_loss: 0.6378 - val_acc: 0.8006\n",
      "Epoch 100/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7660 - acc: 0.7461 - val_loss: 0.7041 - val_acc: 0.7718\n",
      "Epoch 101/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7722 - acc: 0.7460 - val_loss: 0.7123 - val_acc: 0.7753\n",
      "Epoch 102/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7769 - acc: 0.7449 - val_loss: 0.6718 - val_acc: 0.7815\n",
      "Epoch 103/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7832 - acc: 0.7439 - val_loss: 0.6246 - val_acc: 0.7967\n",
      "Epoch 104/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7685 - acc: 0.7470 - val_loss: 0.7098 - val_acc: 0.7806\n",
      "Epoch 105/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7735 - acc: 0.7473 - val_loss: 0.7040 - val_acc: 0.7748\n",
      "Epoch 106/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7794 - acc: 0.7450 - val_loss: 0.7438 - val_acc: 0.7739\n",
      "Epoch 107/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7743 - acc: 0.7464 - val_loss: 0.6541 - val_acc: 0.7878\n",
      "Epoch 108/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7787 - acc: 0.7460 - val_loss: 0.7455 - val_acc: 0.7859\n",
      "Epoch 109/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7830 - acc: 0.7434 - val_loss: 0.7261 - val_acc: 0.7845\n",
      "Epoch 110/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7876 - acc: 0.7417 - val_loss: 0.7789 - val_acc: 0.7675\n",
      "Epoch 111/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7793 - acc: 0.7446 - val_loss: 0.6314 - val_acc: 0.7932\n",
      "Epoch 112/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7884 - acc: 0.7434 - val_loss: 0.6941 - val_acc: 0.7747\n",
      "Epoch 113/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7860 - acc: 0.7428 - val_loss: 0.6764 - val_acc: 0.7822\n",
      "Epoch 114/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7888 - acc: 0.7411 - val_loss: 0.6548 - val_acc: 0.7891\n",
      "Epoch 115/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7912 - acc: 0.7415 - val_loss: 0.6819 - val_acc: 0.7895\n",
      "Epoch 116/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8002 - acc: 0.7373 - val_loss: 0.6710 - val_acc: 0.7865\n",
      "Epoch 117/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7938 - acc: 0.7405 - val_loss: 0.6408 - val_acc: 0.7873\n",
      "Epoch 118/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8012 - acc: 0.7398 - val_loss: 0.6788 - val_acc: 0.7836\n",
      "Epoch 119/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7987 - acc: 0.7397 - val_loss: 0.7126 - val_acc: 0.7881\n",
      "Epoch 120/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8019 - acc: 0.7395 - val_loss: 0.6602 - val_acc: 0.7937\n",
      "Epoch 121/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7961 - acc: 0.7402 - val_loss: 0.6441 - val_acc: 0.7921\n",
      "Epoch 122/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7990 - acc: 0.7378 - val_loss: 0.7037 - val_acc: 0.7724\n",
      "Epoch 123/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8061 - acc: 0.7374 - val_loss: 0.7144 - val_acc: 0.7907\n",
      "Epoch 124/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8048 - acc: 0.7366 - val_loss: 0.7368 - val_acc: 0.7678\n",
      "Epoch 125/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8116 - acc: 0.7360 - val_loss: 0.6834 - val_acc: 0.7816\n",
      "Epoch 126/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8148 - acc: 0.7356 - val_loss: 0.7234 - val_acc: 0.7717\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 17s - loss: 0.8149 - acc: 0.7356 - val_loss: 0.7674 - val_acc: 0.7629\n",
      "Epoch 128/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8177 - acc: 0.7356 - val_loss: 0.7073 - val_acc: 0.7733\n",
      "Epoch 129/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8306 - acc: 0.7290 - val_loss: 0.7001 - val_acc: 0.7675\n",
      "Epoch 130/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8294 - acc: 0.7308 - val_loss: 0.7094 - val_acc: 0.7731\n",
      "Epoch 131/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8279 - acc: 0.7342 - val_loss: 0.7569 - val_acc: 0.7587\n",
      "Epoch 132/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8361 - acc: 0.7285 - val_loss: 0.7724 - val_acc: 0.7717\n",
      "Epoch 133/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8289 - acc: 0.7323 - val_loss: 0.6878 - val_acc: 0.7685\n",
      "Epoch 134/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8367 - acc: 0.7293 - val_loss: 0.7046 - val_acc: 0.7747\n",
      "Epoch 135/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8344 - acc: 0.7276 - val_loss: 0.7696 - val_acc: 0.7462\n",
      "Epoch 136/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8479 - acc: 0.7259 - val_loss: 0.8591 - val_acc: 0.7364\n",
      "Epoch 137/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8459 - acc: 0.7248 - val_loss: 0.7489 - val_acc: 0.7800\n",
      "Epoch 138/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8534 - acc: 0.7238 - val_loss: 0.8552 - val_acc: 0.7573\n",
      "Epoch 139/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8485 - acc: 0.7259 - val_loss: 0.7599 - val_acc: 0.7710\n",
      "Epoch 140/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8641 - acc: 0.7216 - val_loss: 0.8205 - val_acc: 0.7625\n",
      "Epoch 141/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8665 - acc: 0.7206 - val_loss: 0.7256 - val_acc: 0.7715\n",
      "Epoch 142/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8608 - acc: 0.7209 - val_loss: 0.7573 - val_acc: 0.7713\n",
      "Epoch 143/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8738 - acc: 0.7174 - val_loss: 0.8558 - val_acc: 0.7558\n",
      "Epoch 144/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8756 - acc: 0.7156 - val_loss: 0.7695 - val_acc: 0.7629\n",
      "Epoch 145/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8807 - acc: 0.7175 - val_loss: 0.7814 - val_acc: 0.7551\n",
      "Epoch 146/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8882 - acc: 0.7113 - val_loss: 0.8041 - val_acc: 0.7584\n",
      "Epoch 147/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8914 - acc: 0.7128 - val_loss: 0.8024 - val_acc: 0.7364\n",
      "Epoch 148/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9048 - acc: 0.7103 - val_loss: 0.7735 - val_acc: 0.7496\n",
      "Epoch 149/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9085 - acc: 0.7095 - val_loss: 0.7318 - val_acc: 0.7773\n",
      "Epoch 150/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9115 - acc: 0.7092 - val_loss: 0.7848 - val_acc: 0.7631\n",
      "Epoch 151/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9190 - acc: 0.7039 - val_loss: 1.0107 - val_acc: 0.7386\n",
      "Epoch 152/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9258 - acc: 0.7039 - val_loss: 0.8309 - val_acc: 0.7562\n",
      "Epoch 153/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9355 - acc: 0.6996 - val_loss: 0.8734 - val_acc: 0.7398\n",
      "Epoch 154/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9389 - acc: 0.7011 - val_loss: 0.8376 - val_acc: 0.7410\n",
      "Epoch 155/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9458 - acc: 0.7007 - val_loss: 0.8266 - val_acc: 0.7498\n",
      "Epoch 156/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9509 - acc: 0.6976 - val_loss: 0.9753 - val_acc: 0.7112\n",
      "Epoch 157/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9587 - acc: 0.6955 - val_loss: 0.8522 - val_acc: 0.7557\n",
      "Epoch 158/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9676 - acc: 0.6881 - val_loss: 0.8401 - val_acc: 0.7308\n",
      "Epoch 159/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9679 - acc: 0.6917 - val_loss: 0.8123 - val_acc: 0.7590\n",
      "Epoch 160/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9737 - acc: 0.6883 - val_loss: 0.7992 - val_acc: 0.7463\n",
      "Epoch 161/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9904 - acc: 0.6828 - val_loss: 0.9583 - val_acc: 0.7327\n",
      "Epoch 162/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9915 - acc: 0.6848 - val_loss: 0.9057 - val_acc: 0.7338\n",
      "Epoch 163/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9995 - acc: 0.6829 - val_loss: 0.9886 - val_acc: 0.7110\n",
      "Epoch 164/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9971 - acc: 0.6813 - val_loss: 0.8571 - val_acc: 0.7393\n",
      "Epoch 165/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9997 - acc: 0.6815 - val_loss: 0.8993 - val_acc: 0.7184\n",
      "Epoch 166/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0246 - acc: 0.6783 - val_loss: 0.8893 - val_acc: 0.7147\n",
      "Epoch 167/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0261 - acc: 0.6753 - val_loss: 1.0503 - val_acc: 0.7102\n",
      "Epoch 168/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0363 - acc: 0.6689 - val_loss: 1.0228 - val_acc: 0.6933\n",
      "Epoch 169/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0430 - acc: 0.6703 - val_loss: 0.9087 - val_acc: 0.7207\n",
      "Epoch 170/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0462 - acc: 0.6667 - val_loss: 0.8381 - val_acc: 0.7413\n",
      "Epoch 171/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0579 - acc: 0.6657 - val_loss: 0.9427 - val_acc: 0.7243\n",
      "Epoch 172/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0668 - acc: 0.6616 - val_loss: 0.9875 - val_acc: 0.7126\n",
      "Epoch 173/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0666 - acc: 0.6607 - val_loss: 0.8397 - val_acc: 0.7295\n",
      "Epoch 174/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0684 - acc: 0.6614 - val_loss: 1.0430 - val_acc: 0.7004\n",
      "Epoch 175/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0708 - acc: 0.6586 - val_loss: 0.8837 - val_acc: 0.7218\n",
      "Epoch 176/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0768 - acc: 0.6579 - val_loss: 1.1782 - val_acc: 0.6673\n",
      "Epoch 177/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0872 - acc: 0.6572 - val_loss: 1.0771 - val_acc: 0.6730\n",
      "Epoch 178/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0869 - acc: 0.6539 - val_loss: 1.0444 - val_acc: 0.6744\n",
      "Epoch 179/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1017 - acc: 0.6478 - val_loss: 1.1292 - val_acc: 0.6197\n",
      "Epoch 180/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1001 - acc: 0.6529 - val_loss: 1.1734 - val_acc: 0.6630\n",
      "Epoch 181/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1010 - acc: 0.6491 - val_loss: 1.0780 - val_acc: 0.6522\n",
      "Epoch 182/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1115 - acc: 0.6471 - val_loss: 0.8870 - val_acc: 0.7127\n",
      "Epoch 183/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1138 - acc: 0.6482 - val_loss: 1.2253 - val_acc: 0.5950\n",
      "Epoch 184/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1261 - acc: 0.6427 - val_loss: 1.0290 - val_acc: 0.6656\n",
      "Epoch 185/200\n",
      "1562/1562 [==============================] - 18s - loss: 1.1338 - acc: 0.6400 - val_loss: 1.1580 - val_acc: 0.6612\n",
      "Epoch 186/200\n",
      "1562/1562 [==============================] - 18s - loss: 1.1315 - acc: 0.6431 - val_loss: 1.0044 - val_acc: 0.6806\n",
      "Epoch 187/200\n",
      "1562/1562 [==============================] - 18s - loss: 1.1316 - acc: 0.6384 - val_loss: 0.9799 - val_acc: 0.6881\n",
      "Epoch 188/200\n",
      "1562/1562 [==============================] - 18s - loss: 1.1398 - acc: 0.6344 - val_loss: 0.9565 - val_acc: 0.7127\n",
      "Epoch 189/200\n",
      "1562/1562 [==============================] - 18s - loss: 1.1556 - acc: 0.6322 - val_loss: 1.2175 - val_acc: 0.6259\n",
      "Epoch 190/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1517 - acc: 0.6362 - val_loss: 0.9443 - val_acc: 0.6841\n",
      "Epoch 191/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1558 - acc: 0.6349 - val_loss: 1.1098 - val_acc: 0.6727\n",
      "Epoch 192/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1591 - acc: 0.6349 - val_loss: 1.0126 - val_acc: 0.7079\n",
      "Epoch 193/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1580 - acc: 0.6322 - val_loss: 1.1743 - val_acc: 0.6567\n",
      "Epoch 194/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1597 - acc: 0.6331 - val_loss: 1.0294 - val_acc: 0.6765\n",
      "Epoch 195/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1697 - acc: 0.6302 - val_loss: 0.9794 - val_acc: 0.7031\n",
      "Epoch 196/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1662 - acc: 0.6335 - val_loss: 0.9450 - val_acc: 0.7191\n",
      "Epoch 197/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1722 - acc: 0.6266 - val_loss: 0.9444 - val_acc: 0.6868\n",
      "Epoch 198/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1748 - acc: 0.6282 - val_loss: 1.0099 - val_acc: 0.6982\n",
      "Epoch 199/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1827 - acc: 0.6244 - val_loss: 1.1669 - val_acc: 0.6520\n",
      "Epoch 200/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1732 - acc: 0.6284 - val_loss: 0.9671 - val_acc: 0.7015\n",
      "Saved trained model at /mnt/shared/rabbiteer2017/saved_models/keras_cifar10_trained_model.h5 \n"
     ]
    }
   ],
   "source": [
    "print('Train the model.')\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model = keras.models.load_model(model_path)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model with test data set and share sample prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 0.68\n",
      "0. Actual Label = cat vs. Predicted Label = dog\n",
      "1. Actual Label = ship vs. Predicted Label = automobile\n",
      "2. Actual Label = ship vs. Predicted Label = frog\n",
      "3. Actual Label = airplane vs. Predicted Label = frog\n",
      "4. Actual Label = frog vs. Predicted Label = airplane\n",
      "5. Actual Label = frog vs. Predicted Label = frog\n",
      "6. Actual Label = automobile vs. Predicted Label = bird\n",
      "7. Actual Label = frog vs. Predicted Label = bird\n",
      "8. Actual Label = cat vs. Predicted Label = airplane\n",
      "9. Actual Label = automobile vs. Predicted Label = dog\n",
      "10. Actual Label = airplane vs. Predicted Label = horse\n",
      "11. Actual Label = truck vs. Predicted Label = truck\n",
      "12. Actual Label = dog vs. Predicted Label = ship\n",
      "13. Actual Label = horse vs. Predicted Label = truck\n",
      "14. Actual Label = truck vs. Predicted Label = bird\n",
      "15. Actual Label = ship vs. Predicted Label = ship\n",
      "16. Actual Label = dog vs. Predicted Label = airplane\n",
      "17. Actual Label = horse vs. Predicted Label = bird\n",
      "18. Actual Label = ship vs. Predicted Label = cat\n",
      "19. Actual Label = frog vs. Predicted Label = frog\n",
      "20. Actual Label = horse vs. Predicted Label = airplane\n"
     ]
    }
   ],
   "source": [
    "# Load label names to use in prediction results\n",
    "label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "\n",
    "keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "datadir_base = os.path.expanduser(keras_dir)\n",
    "if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join('/tmp', '.keras')\n",
    "label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "\n",
    "with open(label_list_path, mode='rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# Evaluate model with test data set and share sample prediction results\n",
    "evaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n",
    "                                      batch_size=batch_size),\n",
    "                                      steps=x_test.shape[0] // batch_size)\n",
    "\n",
    "print('Model Accuracy = %.2f' % (evaluation[1]))\n",
    "\n",
    "predict_gen = model.predict_generator(datagen.flow(x_test, y_test,\n",
    "                                      batch_size=batch_size),\n",
    "                                      steps=x_test.shape[0] // batch_size)\n",
    "\n",
    "for predict_index, predicted_y in enumerate(predict_gen):\n",
    "    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n",
    "    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n",
    "    print('%d. Actual Label = %s vs. Predicted Label = %s' % (predict_index, actual_label,\n",
    "                                                          predicted_label))\n",
    "    if predict_index == num_predictions:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0be8254250>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRcEvRExR8AsRU/4/IG8SVjwWdJsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c04053990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "imshow(x_test[19], interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
