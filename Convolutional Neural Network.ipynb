{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatx=float32 python cifar10_cnn.py\n",
    "\n",
    "It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n",
    "(it's still underfitting at that point, though)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimensions:(32, 32, 3)\n",
      "Target Dimensions: (10,)\n",
      "Target: [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at our data\n",
    "print(\"Input Dimensions:\" + str(x_train[0].shape))\n",
    "print(\"Target Dimensions: \" + str(y_train[0].shape))\n",
    "print(\"Target: \" + str(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Data Augmentation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Train the model.\n",
      "Epoch 1/200\n",
      "1562/1562 [==============================] - 19s - loss: 1.8453 - acc: 0.3222 - val_loss: 1.5459 - val_acc: 0.4392\n",
      "Epoch 2/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.5666 - acc: 0.4280 - val_loss: 1.3721 - val_acc: 0.5113\n",
      "Epoch 3/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.4349 - acc: 0.4825 - val_loss: 1.2648 - val_acc: 0.5531\n",
      "Epoch 4/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.3453 - acc: 0.5206 - val_loss: 1.1819 - val_acc: 0.5771\n",
      "Epoch 5/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.2695 - acc: 0.5507 - val_loss: 1.0975 - val_acc: 0.6143\n",
      "Epoch 6/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.2076 - acc: 0.5722 - val_loss: 1.0462 - val_acc: 0.6346\n",
      "Epoch 7/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1564 - acc: 0.5914 - val_loss: 1.0292 - val_acc: 0.6374\n",
      "Epoch 8/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1199 - acc: 0.6057 - val_loss: 0.9657 - val_acc: 0.6555\n",
      "Epoch 9/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0817 - acc: 0.6174 - val_loss: 0.9573 - val_acc: 0.6600\n",
      "Epoch 10/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0465 - acc: 0.6318 - val_loss: 0.8938 - val_acc: 0.6834\n",
      "Epoch 11/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0189 - acc: 0.6412 - val_loss: 0.8688 - val_acc: 0.6960\n",
      "Epoch 12/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9934 - acc: 0.6501 - val_loss: 0.8645 - val_acc: 0.6989\n",
      "Epoch 13/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9758 - acc: 0.6581 - val_loss: 0.8355 - val_acc: 0.7096\n",
      "Epoch 14/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9506 - acc: 0.6660 - val_loss: 0.8132 - val_acc: 0.7161\n",
      "Epoch 15/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9380 - acc: 0.6718 - val_loss: 0.8323 - val_acc: 0.7107\n",
      "Epoch 16/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9174 - acc: 0.6773 - val_loss: 0.8080 - val_acc: 0.7179\n",
      "Epoch 17/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9045 - acc: 0.6841 - val_loss: 0.8038 - val_acc: 0.7222\n",
      "Epoch 18/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8950 - acc: 0.6855 - val_loss: 0.7916 - val_acc: 0.7293\n",
      "Epoch 19/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8829 - acc: 0.6935 - val_loss: 0.7401 - val_acc: 0.7457\n",
      "Epoch 20/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8707 - acc: 0.6967 - val_loss: 0.7572 - val_acc: 0.7371\n",
      "Epoch 21/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8531 - acc: 0.7034 - val_loss: 0.7381 - val_acc: 0.7452\n",
      "Epoch 22/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8497 - acc: 0.7035 - val_loss: 0.7452 - val_acc: 0.7451\n",
      "Epoch 23/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8449 - acc: 0.7064 - val_loss: 0.7097 - val_acc: 0.7559\n",
      "Epoch 24/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8414 - acc: 0.7108 - val_loss: 0.7189 - val_acc: 0.7550\n",
      "Epoch 25/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8336 - acc: 0.7114 - val_loss: 0.7194 - val_acc: 0.7604\n",
      "Epoch 26/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8268 - acc: 0.7150 - val_loss: 0.7157 - val_acc: 0.7570\n",
      "Epoch 27/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8219 - acc: 0.7187 - val_loss: 0.6937 - val_acc: 0.7605\n",
      "Epoch 28/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8139 - acc: 0.7201 - val_loss: 0.7089 - val_acc: 0.7626\n",
      "Epoch 29/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8103 - acc: 0.7211 - val_loss: 0.6699 - val_acc: 0.7707\n",
      "Epoch 30/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8088 - acc: 0.7216 - val_loss: 0.6734 - val_acc: 0.7704\n",
      "Epoch 31/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7969 - acc: 0.7264 - val_loss: 0.6788 - val_acc: 0.7676\n",
      "Epoch 32/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7971 - acc: 0.7290 - val_loss: 0.6760 - val_acc: 0.7738\n",
      "Epoch 33/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7921 - acc: 0.7284 - val_loss: 0.6723 - val_acc: 0.7763\n",
      "Epoch 34/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7871 - acc: 0.7301 - val_loss: 0.6712 - val_acc: 0.7691\n",
      "Epoch 35/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7914 - acc: 0.7295 - val_loss: 0.6764 - val_acc: 0.7732\n",
      "Epoch 36/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7784 - acc: 0.7319 - val_loss: 0.6762 - val_acc: 0.7732\n",
      "Epoch 37/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7786 - acc: 0.7352 - val_loss: 0.6529 - val_acc: 0.7818\n",
      "Epoch 38/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7707 - acc: 0.7383 - val_loss: 0.6790 - val_acc: 0.7690\n",
      "Epoch 39/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7706 - acc: 0.7364 - val_loss: 0.6592 - val_acc: 0.7808\n",
      "Epoch 40/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7728 - acc: 0.7363 - val_loss: 0.6473 - val_acc: 0.7845\n",
      "Epoch 41/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7642 - acc: 0.7412 - val_loss: 0.6875 - val_acc: 0.7719\n",
      "Epoch 42/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7676 - acc: 0.7375 - val_loss: 0.6447 - val_acc: 0.7858\n",
      "Epoch 43/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7606 - acc: 0.7424 - val_loss: 0.6586 - val_acc: 0.7798\n",
      "Epoch 44/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7639 - acc: 0.7394 - val_loss: 0.6654 - val_acc: 0.7728\n",
      "Epoch 45/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7583 - acc: 0.7406 - val_loss: 0.6552 - val_acc: 0.7823\n",
      "Epoch 46/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7591 - acc: 0.7425 - val_loss: 0.6294 - val_acc: 0.7893\n",
      "Epoch 47/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7588 - acc: 0.7439 - val_loss: 0.6490 - val_acc: 0.7770\n",
      "Epoch 48/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7545 - acc: 0.7455 - val_loss: 0.6403 - val_acc: 0.7834\n",
      "Epoch 49/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7503 - acc: 0.7442 - val_loss: 0.6551 - val_acc: 0.7800\n",
      "Epoch 50/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7531 - acc: 0.7466 - val_loss: 0.6352 - val_acc: 0.7854\n",
      "Epoch 51/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7514 - acc: 0.7450 - val_loss: 0.6484 - val_acc: 0.7843\n",
      "Epoch 52/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7535 - acc: 0.7431 - val_loss: 0.6130 - val_acc: 0.7948\n",
      "Epoch 53/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7444 - acc: 0.7483 - val_loss: 0.6500 - val_acc: 0.7818\n",
      "Epoch 54/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7432 - acc: 0.7496 - val_loss: 0.6320 - val_acc: 0.7891\n",
      "Epoch 55/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7478 - acc: 0.7472 - val_loss: 0.6685 - val_acc: 0.7907\n",
      "Epoch 56/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7392 - acc: 0.7505 - val_loss: 0.6582 - val_acc: 0.7790\n",
      "Epoch 57/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7472 - acc: 0.7476 - val_loss: 0.6295 - val_acc: 0.7895\n",
      "Epoch 58/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7390 - acc: 0.7503 - val_loss: 0.6766 - val_acc: 0.7880\n",
      "Epoch 59/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7401 - acc: 0.7528 - val_loss: 0.6096 - val_acc: 0.7946\n",
      "Epoch 60/200\n",
      "1562/1562 [==============================] - 18s - loss: 0.7394 - acc: 0.7526 - val_loss: 0.6119 - val_acc: 0.7958\n",
      "Epoch 61/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7442 - acc: 0.7517 - val_loss: 0.6200 - val_acc: 0.7970\n",
      "Epoch 62/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7412 - acc: 0.7510 - val_loss: 0.6393 - val_acc: 0.7879\n",
      "Epoch 63/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7403 - acc: 0.7513 - val_loss: 0.6211 - val_acc: 0.7969\n",
      "Epoch 64/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7336 - acc: 0.7530 - val_loss: 0.6269 - val_acc: 0.8015\n",
      "Epoch 65/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7399 - acc: 0.7510 - val_loss: 0.6410 - val_acc: 0.7879\n",
      "Epoch 66/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7398 - acc: 0.7529 - val_loss: 0.6364 - val_acc: 0.7912\n",
      "Epoch 67/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7410 - acc: 0.7515 - val_loss: 0.6797 - val_acc: 0.7924\n",
      "Epoch 68/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7413 - acc: 0.7517 - val_loss: 0.6512 - val_acc: 0.7941\n",
      "Epoch 69/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7369 - acc: 0.7549 - val_loss: 0.6348 - val_acc: 0.7899\n",
      "Epoch 70/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7417 - acc: 0.7527 - val_loss: 0.6144 - val_acc: 0.7943\n",
      "Epoch 71/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7378 - acc: 0.7542 - val_loss: 0.7468 - val_acc: 0.7730\n",
      "Epoch 72/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7427 - acc: 0.7519 - val_loss: 0.6155 - val_acc: 0.8016\n",
      "Epoch 73/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7398 - acc: 0.7541 - val_loss: 0.6427 - val_acc: 0.7872\n",
      "Epoch 74/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7322 - acc: 0.7569 - val_loss: 0.6772 - val_acc: 0.7821\n",
      "Epoch 75/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7366 - acc: 0.7544 - val_loss: 0.7097 - val_acc: 0.7805\n",
      "Epoch 76/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7443 - acc: 0.7525 - val_loss: 0.6095 - val_acc: 0.7980\n",
      "Epoch 77/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7367 - acc: 0.7550 - val_loss: 0.6402 - val_acc: 0.7851\n",
      "Epoch 78/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7410 - acc: 0.7537 - val_loss: 0.6824 - val_acc: 0.7874\n",
      "Epoch 79/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7413 - acc: 0.7541 - val_loss: 0.6190 - val_acc: 0.7965\n",
      "Epoch 80/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7445 - acc: 0.7529 - val_loss: 0.6775 - val_acc: 0.7836\n",
      "Epoch 81/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7390 - acc: 0.7546 - val_loss: 0.7058 - val_acc: 0.7730\n",
      "Epoch 82/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7428 - acc: 0.7526 - val_loss: 0.6796 - val_acc: 0.7730\n",
      "Epoch 83/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7439 - acc: 0.7517 - val_loss: 0.6360 - val_acc: 0.7907\n",
      "Epoch 84/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7479 - acc: 0.7528 - val_loss: 0.6298 - val_acc: 0.7983\n",
      "Epoch 85/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7477 - acc: 0.7513 - val_loss: 0.6685 - val_acc: 0.7841\n",
      "Epoch 86/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7503 - acc: 0.7498 - val_loss: 0.6033 - val_acc: 0.7957\n",
      "Epoch 87/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7441 - acc: 0.7521 - val_loss: 0.7822 - val_acc: 0.7628\n",
      "Epoch 88/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7497 - acc: 0.7509 - val_loss: 0.6330 - val_acc: 0.7934\n",
      "Epoch 89/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7509 - acc: 0.7513 - val_loss: 0.6881 - val_acc: 0.7743\n",
      "Epoch 90/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7545 - acc: 0.7518 - val_loss: 0.6485 - val_acc: 0.7940\n",
      "Epoch 91/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7517 - acc: 0.7511 - val_loss: 0.6529 - val_acc: 0.7926\n",
      "Epoch 92/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7624 - acc: 0.7465 - val_loss: 0.6548 - val_acc: 0.7965\n",
      "Epoch 93/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7560 - acc: 0.7506 - val_loss: 0.6883 - val_acc: 0.7922\n",
      "Epoch 94/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7574 - acc: 0.7494 - val_loss: 0.6942 - val_acc: 0.7873\n",
      "Epoch 95/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7616 - acc: 0.7483 - val_loss: 0.7418 - val_acc: 0.7708\n",
      "Epoch 96/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7596 - acc: 0.7489 - val_loss: 0.6511 - val_acc: 0.7856\n",
      "Epoch 97/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7643 - acc: 0.7483 - val_loss: 0.6132 - val_acc: 0.7941\n",
      "Epoch 98/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7578 - acc: 0.7521 - val_loss: 0.7040 - val_acc: 0.7758\n",
      "Epoch 99/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7692 - acc: 0.7457 - val_loss: 0.6378 - val_acc: 0.8006\n",
      "Epoch 100/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7660 - acc: 0.7461 - val_loss: 0.7041 - val_acc: 0.7718\n",
      "Epoch 101/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7722 - acc: 0.7460 - val_loss: 0.7123 - val_acc: 0.7753\n",
      "Epoch 102/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7769 - acc: 0.7449 - val_loss: 0.6718 - val_acc: 0.7815\n",
      "Epoch 103/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7832 - acc: 0.7439 - val_loss: 0.6246 - val_acc: 0.7967\n",
      "Epoch 104/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7685 - acc: 0.7470 - val_loss: 0.7098 - val_acc: 0.7806\n",
      "Epoch 105/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7735 - acc: 0.7473 - val_loss: 0.7040 - val_acc: 0.7748\n",
      "Epoch 106/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7794 - acc: 0.7450 - val_loss: 0.7438 - val_acc: 0.7739\n",
      "Epoch 107/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7743 - acc: 0.7464 - val_loss: 0.6541 - val_acc: 0.7878\n",
      "Epoch 108/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7787 - acc: 0.7460 - val_loss: 0.7455 - val_acc: 0.7859\n",
      "Epoch 109/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7830 - acc: 0.7434 - val_loss: 0.7261 - val_acc: 0.7845\n",
      "Epoch 110/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7876 - acc: 0.7417 - val_loss: 0.7789 - val_acc: 0.7675\n",
      "Epoch 111/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7793 - acc: 0.7446 - val_loss: 0.6314 - val_acc: 0.7932\n",
      "Epoch 112/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7884 - acc: 0.7434 - val_loss: 0.6941 - val_acc: 0.7747\n",
      "Epoch 113/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7860 - acc: 0.7428 - val_loss: 0.6764 - val_acc: 0.7822\n",
      "Epoch 114/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7888 - acc: 0.7411 - val_loss: 0.6548 - val_acc: 0.7891\n",
      "Epoch 115/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7912 - acc: 0.7415 - val_loss: 0.6819 - val_acc: 0.7895\n",
      "Epoch 116/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8002 - acc: 0.7373 - val_loss: 0.6710 - val_acc: 0.7865\n",
      "Epoch 117/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7938 - acc: 0.7405 - val_loss: 0.6408 - val_acc: 0.7873\n",
      "Epoch 118/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8012 - acc: 0.7398 - val_loss: 0.6788 - val_acc: 0.7836\n",
      "Epoch 119/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7987 - acc: 0.7397 - val_loss: 0.7126 - val_acc: 0.7881\n",
      "Epoch 120/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8019 - acc: 0.7395 - val_loss: 0.6602 - val_acc: 0.7937\n",
      "Epoch 121/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7961 - acc: 0.7402 - val_loss: 0.6441 - val_acc: 0.7921\n",
      "Epoch 122/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.7990 - acc: 0.7378 - val_loss: 0.7037 - val_acc: 0.7724\n",
      "Epoch 123/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8061 - acc: 0.7374 - val_loss: 0.7144 - val_acc: 0.7907\n",
      "Epoch 124/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8048 - acc: 0.7366 - val_loss: 0.7368 - val_acc: 0.7678\n",
      "Epoch 125/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8116 - acc: 0.7360 - val_loss: 0.6834 - val_acc: 0.7816\n",
      "Epoch 126/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8148 - acc: 0.7356 - val_loss: 0.7234 - val_acc: 0.7717\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 17s - loss: 0.8149 - acc: 0.7356 - val_loss: 0.7674 - val_acc: 0.7629\n",
      "Epoch 128/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8177 - acc: 0.7356 - val_loss: 0.7073 - val_acc: 0.7733\n",
      "Epoch 129/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8306 - acc: 0.7290 - val_loss: 0.7001 - val_acc: 0.7675\n",
      "Epoch 130/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8294 - acc: 0.7308 - val_loss: 0.7094 - val_acc: 0.7731\n",
      "Epoch 131/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8279 - acc: 0.7342 - val_loss: 0.7569 - val_acc: 0.7587\n",
      "Epoch 132/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8361 - acc: 0.7285 - val_loss: 0.7724 - val_acc: 0.7717\n",
      "Epoch 133/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8289 - acc: 0.7323 - val_loss: 0.6878 - val_acc: 0.7685\n",
      "Epoch 134/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8367 - acc: 0.7293 - val_loss: 0.7046 - val_acc: 0.7747\n",
      "Epoch 135/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8344 - acc: 0.7276 - val_loss: 0.7696 - val_acc: 0.7462\n",
      "Epoch 136/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8479 - acc: 0.7259 - val_loss: 0.8591 - val_acc: 0.7364\n",
      "Epoch 137/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8459 - acc: 0.7248 - val_loss: 0.7489 - val_acc: 0.7800\n",
      "Epoch 138/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8534 - acc: 0.7238 - val_loss: 0.8552 - val_acc: 0.7573\n",
      "Epoch 139/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8485 - acc: 0.7259 - val_loss: 0.7599 - val_acc: 0.7710\n",
      "Epoch 140/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8641 - acc: 0.7216 - val_loss: 0.8205 - val_acc: 0.7625\n",
      "Epoch 141/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8665 - acc: 0.7206 - val_loss: 0.7256 - val_acc: 0.7715\n",
      "Epoch 142/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8608 - acc: 0.7209 - val_loss: 0.7573 - val_acc: 0.7713\n",
      "Epoch 143/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8738 - acc: 0.7174 - val_loss: 0.8558 - val_acc: 0.7558\n",
      "Epoch 144/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8756 - acc: 0.7156 - val_loss: 0.7695 - val_acc: 0.7629\n",
      "Epoch 145/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8807 - acc: 0.7175 - val_loss: 0.7814 - val_acc: 0.7551\n",
      "Epoch 146/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8882 - acc: 0.7113 - val_loss: 0.8041 - val_acc: 0.7584\n",
      "Epoch 147/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.8914 - acc: 0.7128 - val_loss: 0.8024 - val_acc: 0.7364\n",
      "Epoch 148/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9048 - acc: 0.7103 - val_loss: 0.7735 - val_acc: 0.7496\n",
      "Epoch 149/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9085 - acc: 0.7095 - val_loss: 0.7318 - val_acc: 0.7773\n",
      "Epoch 150/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9115 - acc: 0.7092 - val_loss: 0.7848 - val_acc: 0.7631\n",
      "Epoch 151/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9190 - acc: 0.7039 - val_loss: 1.0107 - val_acc: 0.7386\n",
      "Epoch 152/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9258 - acc: 0.7039 - val_loss: 0.8309 - val_acc: 0.7562\n",
      "Epoch 153/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9355 - acc: 0.6996 - val_loss: 0.8734 - val_acc: 0.7398\n",
      "Epoch 154/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9389 - acc: 0.7011 - val_loss: 0.8376 - val_acc: 0.7410\n",
      "Epoch 155/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9458 - acc: 0.7007 - val_loss: 0.8266 - val_acc: 0.7498\n",
      "Epoch 156/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9509 - acc: 0.6976 - val_loss: 0.9753 - val_acc: 0.7112\n",
      "Epoch 157/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9587 - acc: 0.6955 - val_loss: 0.8522 - val_acc: 0.7557\n",
      "Epoch 158/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9676 - acc: 0.6881 - val_loss: 0.8401 - val_acc: 0.7308\n",
      "Epoch 159/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9679 - acc: 0.6917 - val_loss: 0.8123 - val_acc: 0.7590\n",
      "Epoch 160/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9737 - acc: 0.6883 - val_loss: 0.7992 - val_acc: 0.7463\n",
      "Epoch 161/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9904 - acc: 0.6828 - val_loss: 0.9583 - val_acc: 0.7327\n",
      "Epoch 162/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9915 - acc: 0.6848 - val_loss: 0.9057 - val_acc: 0.7338\n",
      "Epoch 163/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9995 - acc: 0.6829 - val_loss: 0.9886 - val_acc: 0.7110\n",
      "Epoch 164/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9971 - acc: 0.6813 - val_loss: 0.8571 - val_acc: 0.7393\n",
      "Epoch 165/200\n",
      "1562/1562 [==============================] - 17s - loss: 0.9997 - acc: 0.6815 - val_loss: 0.8993 - val_acc: 0.7184\n",
      "Epoch 166/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0246 - acc: 0.6783 - val_loss: 0.8893 - val_acc: 0.7147\n",
      "Epoch 167/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0261 - acc: 0.6753 - val_loss: 1.0503 - val_acc: 0.7102\n",
      "Epoch 168/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0363 - acc: 0.6689 - val_loss: 1.0228 - val_acc: 0.6933\n",
      "Epoch 169/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0430 - acc: 0.6703 - val_loss: 0.9087 - val_acc: 0.7207\n",
      "Epoch 170/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0462 - acc: 0.6667 - val_loss: 0.8381 - val_acc: 0.7413\n",
      "Epoch 171/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0579 - acc: 0.6657 - val_loss: 0.9427 - val_acc: 0.7243\n",
      "Epoch 172/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0668 - acc: 0.6616 - val_loss: 0.9875 - val_acc: 0.7126\n",
      "Epoch 173/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0666 - acc: 0.6607 - val_loss: 0.8397 - val_acc: 0.7295\n",
      "Epoch 174/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0684 - acc: 0.6614 - val_loss: 1.0430 - val_acc: 0.7004\n",
      "Epoch 175/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0708 - acc: 0.6586 - val_loss: 0.8837 - val_acc: 0.7218\n",
      "Epoch 176/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0768 - acc: 0.6579 - val_loss: 1.1782 - val_acc: 0.6673\n",
      "Epoch 177/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0872 - acc: 0.6572 - val_loss: 1.0771 - val_acc: 0.6730\n",
      "Epoch 178/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.0869 - acc: 0.6539 - val_loss: 1.0444 - val_acc: 0.6744\n",
      "Epoch 179/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1017 - acc: 0.6478 - val_loss: 1.1292 - val_acc: 0.6197\n",
      "Epoch 180/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1001 - acc: 0.6529 - val_loss: 1.1734 - val_acc: 0.6630\n",
      "Epoch 181/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1010 - acc: 0.6491 - val_loss: 1.0780 - val_acc: 0.6522\n",
      "Epoch 182/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1115 - acc: 0.6471 - val_loss: 0.8870 - val_acc: 0.7127\n",
      "Epoch 183/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1138 - acc: 0.6482 - val_loss: 1.2253 - val_acc: 0.5950\n",
      "Epoch 184/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1261 - acc: 0.6427 - val_loss: 1.0290 - val_acc: 0.6656\n",
      "Epoch 185/200\n",
      "1562/1562 [==============================] - 18s - loss: 1.1338 - acc: 0.6400 - val_loss: 1.1580 - val_acc: 0.6612\n",
      "Epoch 186/200\n",
      "1562/1562 [==============================] - 18s - loss: 1.1315 - acc: 0.6431 - val_loss: 1.0044 - val_acc: 0.6806\n",
      "Epoch 187/200\n",
      "1562/1562 [==============================] - 18s - loss: 1.1316 - acc: 0.6384 - val_loss: 0.9799 - val_acc: 0.6881\n",
      "Epoch 188/200\n",
      "1562/1562 [==============================] - 18s - loss: 1.1398 - acc: 0.6344 - val_loss: 0.9565 - val_acc: 0.7127\n",
      "Epoch 189/200\n",
      "1562/1562 [==============================] - 18s - loss: 1.1556 - acc: 0.6322 - val_loss: 1.2175 - val_acc: 0.6259\n",
      "Epoch 190/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1517 - acc: 0.6362 - val_loss: 0.9443 - val_acc: 0.6841\n",
      "Epoch 191/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1558 - acc: 0.6349 - val_loss: 1.1098 - val_acc: 0.6727\n",
      "Epoch 192/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1591 - acc: 0.6349 - val_loss: 1.0126 - val_acc: 0.7079\n",
      "Epoch 193/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1580 - acc: 0.6322 - val_loss: 1.1743 - val_acc: 0.6567\n",
      "Epoch 194/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1597 - acc: 0.6331 - val_loss: 1.0294 - val_acc: 0.6765\n",
      "Epoch 195/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1697 - acc: 0.6302 - val_loss: 0.9794 - val_acc: 0.7031\n",
      "Epoch 196/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1662 - acc: 0.6335 - val_loss: 0.9450 - val_acc: 0.7191\n",
      "Epoch 197/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1722 - acc: 0.6266 - val_loss: 0.9444 - val_acc: 0.6868\n",
      "Epoch 198/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1748 - acc: 0.6282 - val_loss: 1.0099 - val_acc: 0.6982\n",
      "Epoch 199/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1827 - acc: 0.6244 - val_loss: 1.1669 - val_acc: 0.6520\n",
      "Epoch 200/200\n",
      "1562/1562 [==============================] - 17s - loss: 1.1732 - acc: 0.6284 - val_loss: 0.9671 - val_acc: 0.7015\n",
      "Saved trained model at /mnt/shared/rabbiteer2017/saved_models/keras_cifar10_trained_model.h5 \n"
     ]
    }
   ],
   "source": [
    "print('Train the model.')\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model = keras.models.load_model(model_path)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model with test data set and share sample prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 0.68\n",
      "0. Actual Label = cat vs. Predicted Label = dog\n",
      "1. Actual Label = ship vs. Predicted Label = automobile\n",
      "2. Actual Label = ship vs. Predicted Label = frog\n",
      "3. Actual Label = airplane vs. Predicted Label = frog\n",
      "4. Actual Label = frog vs. Predicted Label = airplane\n",
      "5. Actual Label = frog vs. Predicted Label = frog\n",
      "6. Actual Label = automobile vs. Predicted Label = bird\n",
      "7. Actual Label = frog vs. Predicted Label = bird\n",
      "8. Actual Label = cat vs. Predicted Label = airplane\n",
      "9. Actual Label = automobile vs. Predicted Label = dog\n",
      "10. Actual Label = airplane vs. Predicted Label = horse\n",
      "11. Actual Label = truck vs. Predicted Label = truck\n",
      "12. Actual Label = dog vs. Predicted Label = ship\n",
      "13. Actual Label = horse vs. Predicted Label = truck\n",
      "14. Actual Label = truck vs. Predicted Label = bird\n",
      "15. Actual Label = ship vs. Predicted Label = ship\n",
      "16. Actual Label = dog vs. Predicted Label = airplane\n",
      "17. Actual Label = horse vs. Predicted Label = bird\n",
      "18. Actual Label = ship vs. Predicted Label = cat\n",
      "19. Actual Label = frog vs. Predicted Label = frog\n",
      "20. Actual Label = horse vs. Predicted Label = airplane\n"
     ]
    }
   ],
   "source": [
    "# Load label names to use in prediction results\n",
    "label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "\n",
    "keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "datadir_base = os.path.expanduser(keras_dir)\n",
    "if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join('/tmp', '.keras')\n",
    "label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "\n",
    "with open(label_list_path, mode='rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# Evaluate model with test data set and share sample prediction results\n",
    "evaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n",
    "                                      batch_size=batch_size),\n",
    "                                      steps=x_test.shape[0] // batch_size)\n",
    "\n",
    "print('Model Accuracy = %.2f' % (evaluation[1]))\n",
    "\n",
    "predict_gen = model.predict_generator(datagen.flow(x_test, y_test,\n",
    "                                      batch_size=batch_size),\n",
    "                                      steps=x_test.shape[0] // batch_size)\n",
    "\n",
    "for predict_index, predicted_y in enumerate(predict_gen):\n",
    "    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n",
    "    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n",
    "    print('%d. Actual Label = %s vs. Predicted Label = %s' % (predict_index, actual_label,\n",
    "                                                          predicted_label))\n",
    "    if predict_index == num_predictions:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4b585b7910>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b58c53310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "imshow(x_test[5], interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
